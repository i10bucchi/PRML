\chapter{確率分布}

\begin{itemize}
    \item 観測値$\bm{x}_2, ... \bm{x}_N$が与えられた時の確率変数$p(\bm{x})$をモデル化する問題を密度推定という.
    \item 観測されたデータ集合を生成しうる確率分布は無限に考えられるため不良設定.
\end{itemize}

\section{二値変数}

\subsection{ベルヌーイ分布}
確率変数$x$が$x \{0, 1\}$の場合を考えてみる. コイン投げとか.$x=1$となる確率を$\mu$とすると

$$p(x=1|\mu) = \mu$$
$$p(x=0|\mu) = 1 - \mu$$

となる. したがって確率分布は

$$p(x|\mu) = \mu^x(1-\mu)^{1-x}$$

となる. これをベルヌーイ分布という.
$x=1$の時は$\mu^x$が1になる. 逆に$x=0$の時は$(1-\mu)^{1-x}$が1になる.

教科書の式(2.3)と式(2.4)で平均と分散も確認.

次に尤度関数を考えたい. そこでxの観測値のデータ集合$D = \{x_1, ..., x_n\}$があるとする.

すると, 尤度関数は同時確率で表されるため

$$p(D|\mu) = \prod_{n=1}^{N}p(x_n|\mu) = \prod_{n=1}^{N}\mu^{x_n}(1-\mu)^{1-x_n}$$

となる.

対数尤度関数は

$$\ln p(D|x) = \sum_{n=1}^{N} \{\ln x_n\mu + (1-x_n)\ln (1-\mu) \}$$

となる.

この時, $\sum_n x_n$は十分統計量となっている.

十分統計量とは, ある確率分布に対して, その確率分布がもつパラメータ$\theta$がわからなくても, 「その他の値」から関数分布を推定できるような「その他の値」のこと.

例えば, ベルヌーイ分布はパラメータ$\mu$がわからなくとも$\sum_{n} x_n$がわかれば確率分布の形が推定できる.
このサイトがわかりやすい. (https://abicky.net/2014/03/03/202054/)

実際に教科書の式(2.7)からわかるように, 最尤推定の結果$\mu_{ML}$は$\sum_{n=1}^{N} x_n$を使って表される.

\subsection{二項分布}

ベルヌーイ分布は一回試行の確率分布だったため, N回試行に拡張した確率分布である二項分布を見てみる.

$N$回試行して$m$回表が出たとするとその確率は$\mu^{m}(1-\mu)^{N-m}$と表すことができる. 

N回試行の中でm回表が出るパターンは$\frac{N!}{(N-m)!m!}$となる.

よって, N回試行してm回表が出る確率は, ある1パターンにおけるm回表が出る確率に全パターン数をかけて

$$p(m|N, \mu) = \frac{N!}{(N-m)!m!}\mu^{m}(1-\mu)^{N-m}$$

と表すことができる.

平均と分散は教科書を確認.

\subsection{ベータ分布}

ベルヌーイ分布や二項分布は最尤推定を使うと表になる割合が出てくることがわかった.
これは過学習しやすい. 3回中3回が表のコイン投げのデータを使って最尤推定すると$\mu_{ML}=1$となった. (教科書確認)

そこでベイズ的考えを導入する. そのためには事前分布を導入しなければならない. ここで都合のいい事前分布について考えてみる. 

事後分布は尤度と事前分布の積に比例するのであった.

例えば尤度関数が$\mu^x(1-\mu)^{1-x}$のような形になっているなら事前分布を$\mu$もしくは$(1-\mu)$のべき乗になるように選べば, 事後分布は$\mu^{?}(1-\mu)^{?}$といった形になるはず.

これは共益性と呼ばれ色々と都合がいいっぽい. どう都合がいいのかはまだ書かれていない.

ベルヌーイ分布と二項分布に対して共益性のあるベータ分布を導入する.

$$Beta(\mu|a, b) = \frac{\Gamma (a+b)}{\Gamma (a) \Gamma (b)}\mu^{a-1}(1-\mu)^{b-1} $$

係数は正規化されることを保証している.

平均と分散は教科書で確認.

パラメータ$a$, $b$は$\mu$は決めるパラメータなのでハイパラメータと呼ばれる.

二項分布の$\mu$の事後分布は二項分布の尤度関数とベータ分布に比例するので

$$ p(\mu | m, N, a, b) \propto \mu^m(1-\mu)^{N-m} \times \frac{\Gamma (a+b)}{\Gamma (a) \Gamma (b)}\mu^{a-1}(1-\mu)^{b-1} $$

$\mu$に関係のない$\frac{\Gamma (a+b)}{\Gamma (a) \Gamma (b)}$を除外すると

\begin{eqnarray*}
    p(\mu | m, N, a, b) &\propto& \mu^m(1-\mu)^{N-m} \times \mu^{a-1}(1-\mu)^{b-1} \\
                        &\propto& \mu^{m+a-1}(1-\mu)^{N-m+b-1}
\end{eqnarray*}

となる. これもベータ分布とみなせるため, 正規化定数を合わせて

$$ p(\mu | m, l, a, b) = \frac{\Gamma (m+a+l+b)}{\Gamma (m+a) \Gamma (l+b)} \mu^{m+a-1}(1-\mu)^{N-m+b-1}$$

となる.

事前分布から事後分布を求めるには$a$の値を$m$だけ, $b$の値は$l$だけずらせば良いことがわかる.
よって$a$を$m$, $b$を$l$に置き換えたら再び事前分布のように振る舞えるため, 逐次学習を行うことができる.

教科書の図2.2をみるとサンプル数(a+b)が多ければ多いほど分散が小さくなる(確信度が高くなる).
この性質はベイズ推定で平均的に成り立つがもちろん平均的であって全てに対して成り立つわけではない.

\section{多値変数}

二値の場合を見てきたので, 今度はもっと拡張して多値の場合を見たい.
ここで相互に排他的なK個の値を取りうる離散値の確率分布を考えたい.

観測値の表し方はよくOneHotベクトルが用いられる.
要素を$k$と表すとすると$\{0, 1, ..., 5\}$の値を取りうる中で3が観測された場合は$x_3=1$となり, OneHotベクトルは$\bm{x} = \{ 0, 0, 0, 1, 0, 0 \}$と表される.

$x_k = 1$になる確率をパラメータ$\mu_k$で表すと

$$ p(\bm{x}|\bm{\mu}) = \prod_{k=1}^K \mu_k^{x_k} $$

ここでN個の独立な観測値$\bm{x_1}, ... \bm{x_N}$を観測したとする.

この時の尤度関数は

$$ \prod_{n=1}^N\prod_{k=1}^K\mu_k^{x_{nk}} = \prod_{k=1}^K\mu_k^{(\sum_{n=1}^Nx_{nk})} = \prod_{k=1}^K\mu_k^{m_k} $$

となる.

ベルヌーイ分布と同様に$m_k$が十分統計量となっている.

ラグランジュ乗数法を用いて対数尤度関数を最大化でき, その結果は

$$ \mu_k^{ML} = \frac{m_k}{N} $$

となる.

また, 1回試行ではなくN回試行の場合もベルヌーイ分布と二項分布の関係のように考えることができ,

$$ \frac{N!}{m_1!m_2!...m_k!}\prod_{k=1}^K \mu_k^{m_k} $$

となる. これは多項分布と呼ばれる.

\subsection{ディリクレ分布}

多項式分布の事前分布を導入する.

多項式分布の形から共益分布は

$$ p(\bm{\mu})|\bm{\alpha}) \propto \prod_{k=1}^K \mu_k^{\alpha_k - 1} $$

これを正規化すると

$$ Dir(\bm{\mu}|\bm{\alpha}) = \frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1)....\Gamma(\alpha_K)}\prod_{k=1}^K \mu_k^{\alpha_k - 1} $$

事後分布を計算したいので事前分布と尤度関数の積を取ると

\begin{eqnarray*}
    p(\bm{\mu}|D, \alpha) &\propto& \frac{N!}{m_1!m_2!...m_k!}\prod_{k=1}^K \mu_k^{m_k} \times \frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1)....\Gamma(\alpha_K)}\prod_{k=1}^K \mu_k^{\alpha_k - 1} \\
    &\propto& \prod_{k=1}^K \mu_k^{m_k} \times \prod_{k=1}^K \mu_k^{\alpha_k - 1} \\
    &\propto& \prod_{k=1}^K \mu_k^{\mu_k + \alpha_k - 1}
\end{eqnarray*}

この事後分布はまた新たにディリクレ分布の事前分布として使用できる形になっていることがわかる.
また, $\alpha$は$m$と一緒に指数部分にあるため, $x_k=1$の有効観測数となっている.

\section{ガウス分布}

ガウス分布は, 正規分布とも呼ばれ, 連続変数の分布のモデルとして多く利用される。

変数が$x$の1つ場合のガウス分布, $\mu$は平均で$\sigma^{2}$は分散を表す.

$$ N\left({x|\mu,\sigma^{2}}\right) = \frac{1}{\left(2\pi\sigma\right)^\frac{1}{2}}\exp^{\left\{{-\frac{1}{2\sigma^{2}}\left(x-\mu\right)^{2}}\right\}} $$

$\bm{D}$次元ベクトル$x$に対するガウス分布, $\mu$は平均ベクトル, $\Sigma$は共分散行列を表す.

$$ N\left(\bm{x|\mu,\Sigma}\right) = \frac{1}{\left(2\pi\right)^\frac{\bm{D}}{2}}\frac{1}{\left|\Sigma\right|^\frac{1}{2}}\exp^{\left\{{-\frac{1}{2}\left(\bm{x}-\bm{\mu}\right)^{T}\Sigma^{-1}\left(\bm{x}-\bm{\mu}\right)}\right\}} $$

ガウス分布が使われる場面の例として,以下がある.
\begin{itemize}
  \item 変数が1つのときのに, エントロピーを最大化する分布
  \item 確率変数の和における変数数が増えるに従って近づく分布(中心極限定理)
  \item ガウス分布を用いたマルコフ確率場
  \item 時系列データのモデル化に用いられる線形動的システム
\end{itemize}

　ガウス分布は, 指数部分に現れる二次形式$ \triangle^2 = {\left\{{\left(\bm{x}-\bm{\mu}\right)^{T}\Sigma^{-1}\left(\bm{x}-\bm{\mu}\right)}\right\}} $に依存する. この部分が定数の時, ガウス分布の密度が一定になる.
係数部分はガウス分布を正規化するもの.

　多変量ガウス分布をD個の独立な1変数ガウス分布の積で表すことが出来る.

　ガウス分布のパラメータの総数はDに対して2乗の割合で増加し, 計算が困難になりえる. 対処法の1つは, 共分散行列を対角化するなどで形式に制限を加えることである. また, ガウス分布は極大値が1つという特徴があるため, 適切に分布を表現できない際に, 洗剤変数を導入する場合がある. 例として, ガウス混合分布などがある.

\subsection{条件付きガウス分布}
　多変量ガウス分布の重要な特性として, 2つの変数集合の同時分布がガウス分布に従うなら, 一方の変数集合が与えられたときの, もう一方の集合の条件付き分布もガウス分布になる. さらに, どちらの変数集合の周辺分布も同様にガウス分布になる. 

大まかな証明の流れを記述しておくので, 詳細は教科書を参照.

基本的には, $\bm{x}$の二次の項の係数行列は逆共分散行列$\Sigma^{-1}$に等しく, 線形項の係数と$\Sigma^{-1}\mu$が等しいことを利用して, $\Sigma$と$\mu$が得られる. 

これらの演算は, 平方完成と呼ばれるものの例となっており, ガウス分布についてはよく使われる技法である.

 二次形式とは, 二次の項のみからなる多項式であり, 変数行列$\bm{A}$と「変数を縦に並べたベクトル$\bm{x}$」を用いて, $\bm{x}^{T}\bm{A}\bm{x}$と表すことが出来る.

\subsection{周辺ガウス分布}
　同時分布$p(\bm{x}_{a}, \bm{x}_{b})$がガウス分布であれば, 条件付き分布$p(\bm{x}_{a}|\bm{x}_{b})$もガウス分布になることを示した. ここでは, 周辺分布
$$ p(\bm{x}_{a}) = \int_p(\bm{x}_{a}, \bm{x}_{b})d\bm{x}_{b} $$ 
であることを証明する.

前項と同じほとんど同じ手順で証明する事ができる. 詳細は教科書を参照.
\\

分割されたガウス分布について, 以下にまとめる.

$A = \Sigma$とした, 同時ガウス分布$ N(\bm{x}|\bm{\mu}, \bm{\Sigma}) $があるとする.

$$ \bm{X} = \left(\begin{array}{c}\bm{x}_{a}\\ \bm{x}_{b}\end{array}\right), \bm{\mu} = \left(\begin{array}{c}\bm{\mu}_{a}\\ \bm{\mu}_{b}\end{array}\right)$$

$$ \bm{\Sigma} = \left(\begin{array}{cc}\bm{\Sigma}_{aa} & \bm{\Sigma}_{ab}\\ \bm{\Sigma}_{ba} & \bm{\Sigma}_{bb}\end{array}\right), \bm{\Lambda} =  \left(\begin{array}{cc}\bm{\Lambda}_{aa} & \bm{\Lambda}_{ab}\\ \bm{\Lambda}_{ba} & \bm{\Lambda}_{bb}\end{array}\right)$$

条件付き分布:

$$ p(\bm{x}_{a}|\bm{x}_{b}) = \bm{N}(\bm{x}_{a}|\bm{\mu}_{a|b}, \bm{\Lambda}_{aa}^{-1})  $$
$$ \bm{\mu}_{a|b} = \bm{\mu}_{a} - \bm{\Lambda}_{aa}^{-1} \bm{\Lambda}_{ab}(\bm{x}_{a} - \bm{\mu}_{b}) $$

周辺分布:
$$ p(\bm{x}_{a}) = \bm{N}(\bm{x}_{a}|\bm{\mu}_{a}, \bm{\Sigma}_{aa})  $$


\subsection{ガウス変数に対するベイズの定理}
ガウス分布$p(\bm{x})$と, $p(\bm{x})$が与えられたときの $p(\bm{y|x})$の条件付きガウス分布が次式で与えられたときの, $p(\bm{y})$と $p(\bm{x|y})$について考える.
$$p(\bm{x}) = \bm{N}(\bm{x}|\bm{\mu}, \bm{\Lambda}^{-1})$$
$$p(\bm{y|x}) = \bm{N}(\bm{x}|\bm{Ax}+\bm{b}, \bm{L}^{-1})$$
が与えられた時
$$p(\bm{y}) = \bm{N}(\bm{y}|\bm{A}\bm{\mu}+\bm{b}, \bm{L}^{-1}+\bm{\Lambda}\bm{\Lambda}^{-1}\bm{\Lambda}^{T})$$
$$p(\bm{x|y}) = \bm{N}(\bm{x}|\bm{A}\bm{\mu}\left\{\bm{A}^{T}\bm{L}(\bm{y}-\bm{b})\right\} + \bm{\Lambda}\bm{\mu}, \bm{\Sigma})$$
$$\bm{\Sigma} = ({\Lambda}+\bm{A}^{T}\bm{L}\bm{A})^{-1}$$

\subsection{ガウス分布の最尤推定}
多変量ガウス分布から、観測値$\bm{x_n}$が独立に得られたと仮定したデータ集合$\bm{X}$があるとき、最尤推定法によって分布を求める。

対数尤度関数は
$$ \ln p(\bm{X}|\bm{\mu}, \bm{\Sigma}) =  -\frac{\bm{ND}}{2}\ln(2\pi)-\frac{1}{2}\sum_{n=1}^N(\bm{x_n}-\bm{\mu})^{T}\Sigma ^{-1}(\bm{x_n}-\bm{\mu})$$

十分統計量は

$$ \sum_{n=1}^N\bm{x_n},  \sum_{n=1}^N\bm{x_n}\bm{x_n}^{T} $$

対数尤度の$\mu$についての導関数を考えると

$$ \frac{\partial}{\partial \mu}\ln p(\bm{X}|\bm{\mu}, \bm{\Sigma}) = \sum_{n=1}^N\Sigma ^{-1}(\bm{x_n}-\bm{\mu})$$

となり、この導関数を0とおくと、最尤推定による平均についての解が得られる。

$$ \bm{\mu_{ML}} = \frac{1}{\bm{N}}$$
\subsection{逐次推定}
ガウス分布のパラメータの最尤推定解についての議論を基に、最尤推定での逐次推定という一般的な議論に展開する。

平均の最尤推定量$\bm{\mu_{ML}} $に対して、最後のデータ点$\bm{X_N}$がどれくらい影響したかを指揮に表すと

$$ \bm{\mu_{ML}} = \bm{\mu_{ML}}^{(\bm{M}-1)} + \frac{1}{N}(\bm{X_N}-\bm{\mu_{ML}}^{(\bm{M}-1)})$$

これは、新しいデータ店を観測すると$\frac{1}{N}$に比例する小さな量だけ、誤差信号の方へ、古い推定量を移動させて、$\bm{\mu_{ML}}$を修正したと解釈できる

上式を利用した逐次アルゴリズムで導出することは、いつもできるわけではなく、

より汎用的な逐次学習の定式化が必要なため、Robbins-Menroアルゴリズムが使用される。

同時分布$\bm{p}(\bm{z}, \bm{\theta})$いしたがくｋアク率変数$\bm{\theta}$と$\bm{z}$の対を考える。$\bm{\theta}$が与えられたときの$\bm{z}$の条件付き期待値によって、決定論的な関数$f(\bm{\theta})$を定義する。

$$ f(\bm{\theta}) = E[\bm{z}|\bm{\theta}] = \int zp(\bm{z}|\bm{\theta})dz $$

このように定義された関数を回帰関数という。

いくつかの仮定(教科書参照)のもと、Robbins-Menroの手続きでは、$f(\bm{\theta})=0$となる点の連続的推定の系列を、次のように定義する。

$$ \bm{\theta}^{(N)} = \bm{\theta}^{(N-1)} - a_{N-1}\bm{z}(\bm{\theta}^{(N-1)})$$

$\bm{z}(\bm{\theta}^{(N)})$は$\bm{\theta}$が$\bm{\theta}^{N}$を取るときに観測される$\bm{z}$の値で、係数$a_{\bm{N}}$は、以下の条件を満たす整数の系列である。

$$\lim_{N \rightarrow \infty}a_{\bm{N}} = 0$$
$$ \sum_{N=1}^\infty a_{\bm{N}} = \infty $$
$$ \sum_{N=1}^\infty a^{2}_{\bm{N}} < \infty $$

\newpage