\chapter{序論}

\begin{itemize}
    \item 機械学習とは観測値$\bm{x}$を入力して目的変数$\bm{t}$を出力するような$y(\bm{x})$関数を求めるアルゴリズム
\end{itemize}

\section{例: 多項式曲線フィッティング}

xを入力しtを予測する問題があったとする.

そのための学習データ$\{ \bm{x}, \bm{t} \}$を以下の式でフィッティングすることを考えたい.

$$ t = y(x, \bm{w}) $$

$$ y(x, \bm{w}) = \sum^{M}_{j=0}w_{j}x^j$$

これは

$$ E(\bm{w}) = \frac{1}{2}\sum^{N}_{i=0}(y(x_i, \bm{w}) - t_i)^2$$

を誤差関数として最小化することで達成できる.

(1)式は$\bm{w}$に関して線形なため線形モデルと呼ばれ. 誤差関数は$\bm{w}$の2次関数になっているため, 下に凸の関数系でただ一点の最小値を得ることができる.

$M$はどこまでの次元を考慮するかで決まる. これをモデル選択と言う.

$M$が大きいとオーバーフィッティングしてしまう.(教科書の図1.3を参照)

直感的には2次関数の係数が大きいほど関数は細いU字になる. 
全ての点を通るには細い曲線を描かなければならないため係数が大きくなる.

閉じた式は有限個の初等関数で表せる式. (closed form expletion)

平均二条平方根誤差を導入することで目的変数と同じ尺度で誤差を測れる.
平均二条平方根誤差は

$$ E_{RMS} = \sqrt{2E(\bm{w}*)/N} $$

で表される.

オーバーフィッティングを避けるために正則化と呼ばれるテクニックがある.
これは誤差関数に係数$w$の大きさに応じた罰金項をを与えるもので以下のように表される.

$$ E(\bm{w}) = \frac{1}{2}\sum^{N}_{i=0}(y(x_i, \bm{w}) - t_i)^2 + \frac{\lambda}{2}||\bm{w}||^2 $$

$$ ||\bm{w}||^2 = \bm{w}\bm{w}^T $$

$\lambda$を調整することでオーバーフィッティングを抑えられる.(教科書の図1.7を参照)

\section{確率論}

以下の定理が重要. 今後も出てくる.

加法定理

$$ p(X) = \sum_{Y} p(X, Y) $$

まず$X$が観測される確率を知りたい. しかし, 手元には$X$と$Y$が同時に出現する確率しかない. なので, 全ての$Y$について足せば全体のうち$X$が観測される確率がわかる.

乗法定理

$$ p(X, Y) = p(Y|X)p(X) $$

$X$と$Y$が同時観測される確率を知りたいならば, $Y$のもとで$X$が起きる確率と, そもそも$X$が観測される確率を考える.

$p(X, Y) = p(Y, X)$より

$$ p(Y|X)p(X) = p(X|Y)p(Y) $$

であり,

$$ p(Y|X) = \frac{p(X|Y)p(Y)}{p(X)} $$

となり, ベイズの定理を得る. 
分母の$p(X)$をさらに展開し,

$$ p(Y|X) = \frac{p(X|Y)p(Y)}{\sum_{Y} p(X, Y)} = \frac{p(X|Y)p(Y)}{\sum_{Y} p(X|Y)p(Y)} $$

とも表すことができる. これは全ての$Y$のパターン分のある$Y$のパターンとなっており, どのような場合でも1に収まるような規格化を分母が行なっていることがわかる.

\subsection{確率密度}

離散的な集合から与えられる確率分布の計算と考え方は同じ. ただし, 連続地であるため足し算ではなく積分を使用する.

$$ p_y(y) = p_x(g(y))|g'(y)| $$

教科書の式1.27(上記の式)が示していることは, ヤコビ行列で座標変換を考えると, 変換した座標の拡大率(ヤコビ行列式)の分, 確率の最大値が変わるということ. つまり, 変数の選び方によって, 確率密度の最大値は変わり得る.


% \begin{itembox}[l]{ヤコビ行列についての補足}

% 座標変換は行列で基本的に表される. 行列の中身はどの変数に対してどれだけ変化させるのか(係数)が記載される. 多項式の1つの項に1つの変数が割り当てられることが最もシンプルな変数変換になるが, 1つの項に複数の変数があったり, それらが非線形だったりすることがある. このような場合, どのような行列を設定できるか. 行列の中身は変数に対してどれだけ変化させるのか, つまり微分になることがわかる. よって, 各変数について微分したもので行列を作ると非線形な場合などにも対応できる. これがヤコビ行列. 多分, ヤコビ行列は普通の変換行列の一般形.

% \end{itembox}

確率密度は確率なので多変数だろうが何だろうが, 積分して1にならなければならない.
また, どのような地点においても非負である必要もある.

\subsection{期待値と分散}

期待値とは関数$f(x)$の$x$が$p(x)$で与えられる時の平均のことを言う.
以下のように表す.

$$ \bm{E}[f] = \sum_{i=0}^{N}p(x_i)f(x_i) $$

$$ \bm{E}[f] = \int_{-\infty}^{\infty}p(x)f(x)dx $$

分散とは期待値との2乗誤差の期待値であり

$$ var[f] = \bm{E}[ (f(x) - \bm{E}[f])^2 ] $$

と表すことができる.
式変形によって,

$$ var[f] = \bm{E}[f(x^2)] - \bm{E}[f(x)]^2 $$

$$ var[f] = \bm{E}[x^2] - \bm{E}[x]^2 $$

と表される.

多変数による分散は共分散と呼ばれる.

\subsection{ベイズ確率}

目標変数$t$は一般に様々な影響によりノイズが含まれる. 

そこで, パラメータ$w$の不確実性を取り扱い, 定量的に評価したい!

これは, 確率の枠組みで評価することができる.

教科書の箱の例と同様に, $D$を観測することでそこから与えられる事後分布$p(w|D)$を評価できる.

$$ p(w|D) = \frac{p(D|w)p(w)}{p(D)} $$

右辺にある$p(D|w)$は尤度である.

頻度主義で用いられる最尤推定はこの尤度を最大化するような$w$を選ぶことでパラメータ推定を行う.

しかし, これではオーバーフィッテングを起こしやすい.

\subsection{ガウス分布}

ガウス分布は単一の実数値$x$に対して以下のように定義される.

$$ N(x|\mu, \sigma^2) = \frac{1}{(2\pi\sigma^2)^{1/2}}exp\{ -\frac{1}{2\sigma^2}(x-\mu)^2 \} $$

$\mu$は平均, $\sigma^2$は分散であり$\sigma$は標準偏差となる.

また分散の逆数は, $\beta = 1/\sigma^2$であり精度パラメータと呼ばれる.

多変数の場合は,

$$ N(\bm{x}|\bm{\mu}, \bm{\Sigma}) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\bm{\Sigma}^{1/2}|}exp\{ -\frac{1}{2}(\bm{x} - \bm(\mu)^T\bm{\Sigma}^{-1}(\bm{x} - \bm{\mu})) \} $$

ガウス分布の尤度を考えると, 

$$ p(\bm{x}|\mu, \sigma) = \prod_{i=1}^N p(x_i | \mu, \sigma) $$

となる.

これは計算機のアンダーフローを招いたり計算しづらかったり色々不便なので対数をとって対数尤度として扱うことが多い. 対数をとると尤度関数は以下のようになる.

$$ ln(p(\bm{x}|\mu, \sigma)) = -\frac{1}{2\sigma^2}\sum^N_{n=1}(x_n-\mu)^2 - \frac{N}{2}ln(\sigma^2) - \frac{N}{2}ln(2\pi) $$

これを最尤推定すると,

$\mu_{ML}$は観測値集合の平均, $\sigma_{ML}$は

$$ \sigma_{ML} = \frac{1}{N}\sum_{n=1}^{N}(x_n - \mu_{ML})^2 $$

となる.

最尤推定は分散が$\frac{N-1}{N}$倍過小評価する傾向がある.データ数が多くなるとこの過小評価はあまり影響しない.

\subsection{曲線フィッティング再訪}

曲線フィッティングの目標は, 観測値$\bm{x}$を目標値$\bm{t}$に基づいて, 新たな観測値$x$に対して適切な$t$を求めることにある.

ここで不確実性を確率分布を用いて導入するために, 与えられた$x$に対する, $t$は$y(x)$を平均とするガウス分布に従うものとする.

つまり,

$$ p(t|x, \bm{w}, \beta) = N(t|y(x, \bm{w}, \beta^{-1})) $$

となる.
最尤推定するために尤度関数を導入すると,

$$ p(\bm{t}|\bm{x}, \bm{w}, \beta) = \prod_{n=1}^N p(t_n|y(x_n, \bm{w}), \beta^{-1}) $$

となり, 例によって対数を取る.

$$ ln(p(\bm{t}|\bm{x}, \bm{w}, \beta)) = -\frac{\beta}{2}\sum^N_{n=1}(y(x_n, \bm{w})-t_n)^2 + \frac{N}{2}ln(\beta) - \frac{N}{2}ln(2\pi) $$

この対数を最大化する$\bm{w}$を考える. 第二項, 第三項は$w$に関係ないので定数としてみて良い. 第一項の$\beta$は定数倍しているだけと考えられるので最大化の場合は関係ない. これらを除外すると, 二乗和誤差の最小化と等価になる.

$\beta$に関しても今までと同様

$$ \frac{1}{\beta} = \frac{1}{N}\sum_{n=1}^{N}(y(x_n, \bm{w}) - t_n)^2 $$

と得ることができる.

よりベイズ的に事前分布の導入を試みたい.
事前分布は簡単かして

$$ p(\bm(w)|\alpha) = N(\bm{w}|0, \alpha^{-1} I) = (\frac{\alpha}{2\pi})^{(M+1)/2}exp\{ -\frac{\alpha}{2}\bm{w}\bm{w}^T \} $$

と表してみる.$\alpha$は精度パラメータ.

ベイズの定理より$\bm{w}$は, 事後確率

$$ p(\bm{w}|\bm{x}, \bm{t}, \alpha, \beta) \propto p(\bm{t}|\bm{x}, \bm{w}, \beta)p(\bm{w}|\alpha) $$

を最大化することで得られる.

これは対数をとって計算すると

$$ \frac{\beta}{2}\sum_{n=0}^N\{ y(x_n, \bm{w}) - t_n \} + \frac{\alpha}{2}\bm{w}\bm{w}^T $$

となる. これは正則化された二乗和誤差の最小化となることに注意する.

\subsection{ベイズ曲線フィッティング}

事前確率を導入したけど, やってることは点推定. 完全なベイズを導入するために, $p(t|x, \bm{x}, \bm{t})$を評価してみる.

$$ p(t|x, \bm{x}, \bm{t}) = \int p(t|x,\bm{w})p(\bm{w}|\bm{x}, \bm{t})dw $$

この式は$t$の確率を求めるために, $w$の全ての可能性の期待値を考えている.もちろん$w$はデータ集合$\{\bm{x}, bm{t}\}$から推測されるもの.


\newpage